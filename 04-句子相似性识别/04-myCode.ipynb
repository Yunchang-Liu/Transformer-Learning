{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 导入需要的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel, AdamW, get_linear_schedule_with_warmup\n",
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # 避免Warning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (/home/yunchang/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2b84f9f3ac94b38a9fcc72105e0939e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 加载mrpc数据集\n",
    "dataset = load_dataset('glue', 'mrpc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>label</th>\n",
       "      <th>idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Amrozi accused his brother , whom he called \" ...</td>\n",
       "      <td>Referring to him as only \" the witness \" , Amr...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yucaipa owned Dominick 's before selling the c...</td>\n",
       "      <td>Yucaipa bought Dominick 's in 1995 for $ 693 m...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>They had published an advertisement on the Int...</td>\n",
       "      <td>On June 10 , the ship 's owners had published ...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Around 0335 GMT , Tab shares were up 19 cents ...</td>\n",
       "      <td>Tab shares jumped 20 cents , or 4.6 % , to set...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The stock rose $ 2.11 , or about 11 percent , ...</td>\n",
       "      <td>PG &amp; E Corp. shares jumped $ 1.63 or 8 percent...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sentence1  \\\n",
       "0  Amrozi accused his brother , whom he called \" ...   \n",
       "1  Yucaipa owned Dominick 's before selling the c...   \n",
       "2  They had published an advertisement on the Int...   \n",
       "3  Around 0335 GMT , Tab shares were up 19 cents ...   \n",
       "4  The stock rose $ 2.11 , or about 11 percent , ...   \n",
       "\n",
       "                                           sentence2  label  idx  \n",
       "0  Referring to him as only \" the witness \" , Amr...      1    0  \n",
       "1  Yucaipa bought Dominick 's in 1995 for $ 693 m...      0    1  \n",
       "2  On June 10 , the ship 's owners had published ...      1    2  \n",
       "3  Tab shares jumped 20 cents , or 4.6 % , to set...      0    3  \n",
       "4  PG & E Corp. shares jumped $ 1.63 or 8 percent...      1    4  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = dataset['train'].to_pandas()\n",
    "df_val = dataset['test'].to_pandas()\n",
    "df_test = dataset['validation'].to_pandas()\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "标准数据集一般无需数据预处理"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 数据集定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, max_len, with_labels=True, bert_model='albert-base-v2'):\n",
    "        # super().__init__()\n",
    "        self.data = data\n",
    "        self.max_len = max_len\n",
    "        self.with_labels = with_labels  # 区分训练集和测试集\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(bert_model)  # 这一步很费时，应该创建对象时就加载\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sentence1 = self.data.loc[index, 'sentence1']\n",
    "        sentence2 = self.data.loc[index, 'sentence2']\n",
    "\n",
    "        encodings = self.tokenizer(\n",
    "            sentence1, sentence2,  # tokenizer可以接受1或2个序列，这里输入两个\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        input_ids = encodings['input_ids'].squeeze(0)  # squeeze(0)表示去除第一个维度。由于输出是二维tensor，[[ ]]，第一维被去除\n",
    "        attention_mask = encodings['attention_mask'].squeeze(0)\n",
    "        token_type_ids = encodings['token_type_ids'].squeeze(0)\n",
    "        \n",
    "        if self.with_labels:  # 若有标签，即训练集\n",
    "            label = self.data.loc[index, 'label']\n",
    "            return input_ids, attention_mask, token_type_ids, label\n",
    "        else:  # 测试集 \n",
    "            return input_ids, attention_mask, token_type_ids"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 模型定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentencePairClassifier(nn.Module):\n",
    "    def __init__(self, bert_model='albert-base-v2', freeze_bert=True):\n",
    "        super().__init__()\n",
    "        self.bert_layer = AutoModel.from_pretrained(bert_model)\n",
    "\n",
    "        #  encoder 隐藏层大小\n",
    "        if bert_model == \"albert-base-v2\":  # 12M 参数\n",
    "            hidden_size = 768\n",
    "        elif bert_model == \"albert-large-v2\":  # 18M 参数\n",
    "            hidden_size = 1024\n",
    "        elif bert_model == \"albert-xlarge-v2\":  # 60M 参数\n",
    "            hidden_size = 2048\n",
    "        elif bert_model == \"albert-xxlarge-v2\":  # 235M 参数\n",
    "            hidden_size = 4096\n",
    "        elif bert_model == \"bert-base-uncased\": # 110M 参数\n",
    "            hidden_size = 768\n",
    "        elif bert_model == \"roberta-base\": # \n",
    "            hidden_size = 768\n",
    "\n",
    "        if freeze_bert:  # 固定Bert层 更新分类输出层\n",
    "            for p in self.bert_layer.parameters():\n",
    "                p.requires_grad = False\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.linear = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    \n",
    "    @autocast()  # 混合精度训练\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        # BERT的输出: last_hidden_state, pooler_output, (hidden_states)可选, (attentions)可选\n",
    "        # last_hidden_state维度：(batch_size, sequence_length, hidden_size), \n",
    "        # pooler_output维度：(batch_size, hidden_size) [CLS]的最后一层的隐藏状态\n",
    "        outputs = self.bert_layer(input_ids, attention_mask, token_type_ids)\n",
    "        logits = self.linear(self.dropout(outputs['pooler_output']))\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    \"\"\" 固定随机种子，保证结果复现\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "\n",
    "def evaluate_loss(net, device, loss_fun, dataloader):\n",
    "    \"\"\"\n",
    "    评估输出\n",
    "    \"\"\"\n",
    "    net.eval()\n",
    "\n",
    "    mean_loss = 0\n",
    "    count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for it, (input_ids, attention_mask, token_type_ids, label) in enumerate(tqdm(dataloader)):\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            token_type_ids = token_type_ids.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            logits = net(input_ids, attention_mask, token_type_ids)\n",
    "            mean_loss += loss_fun(logits.squeeze(-1), label.float()).item()  # squeeze(-1)去除最后维度值为1的维度\n",
    "            count += 1\n",
    "\n",
    "    return mean_loss / count            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bert(net, loss_fun, opti, lr, lr_scheduler, train_loader, val_loader, epochs, iters_to_accumulate, device):\n",
    "    \n",
    "    best_loss = np.Inf  # 最小损失\n",
    "    best_epoch = 1         # 损失最小的epoch序列\n",
    "    nb_iterations = len(train_loader)  # 一个epoch有多少次迭代\n",
    "    print_every = nb_iterations // 5  # 打印频率\n",
    "    iters = []\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    scaler = GradScaler()  # ????????????\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        net.train()\n",
    "        running_loss = 0.0\n",
    "        for it, (input_ids, attention_mask, token_type_ids, label) in enumerate(tqdm(train_loader)):\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            token_type_ids = token_type_ids.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            # 混合精度加速训练 \n",
    "            with autocast(): # ?????\n",
    "                logits = net(input_ids, attention_mask, token_type_ids)\n",
    "                loss = loss_fun(logits.squeeze(-1), label.float())\n",
    "                loss = loss / iters_to_accumulate  # Normalize the loss because it is averaged  ??????????????\n",
    "            \n",
    "            scaler.scale(loss).backward()  # ????????????\n",
    "\n",
    "            # 以下？？？\n",
    "            if (it + 1) % iters_to_accumulate == 0:  # iters_to_accumulate次迭代 更新一次梯度\n",
    "                # Optimization step\n",
    "                # scaler.step() first unscales the gradients of the optimizer's assigned params.\n",
    "                # If these gradients do not contain infs or NaNs, opti.step() is then called,\n",
    "                # otherwise, opti.step() is skipped.\n",
    "                scaler.step(opti)\n",
    "                # Updates the scale for next iteration.\n",
    "                scaler.update()\n",
    "                # 根据迭代次数调整学习率。\n",
    "                lr_scheduler.step()\n",
    "                # 梯度清零\n",
    "                opti.zero_grad()\n",
    "\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "\n",
    "            if (it + 1) % print_every == 0:  # 打印训练损失\n",
    "                print()\n",
    "                print(\"Iteration {}/{} of epoch {} complete. Loss : {} \"\n",
    "                    .format(it+1, nb_iterations, epoch+1, running_loss / print_every))\n",
    "\n",
    "                running_loss = 0.0\n",
    "\n",
    "        val_loss = evaluate_loss(net, device, loss_fun, val_loader)\n",
    "        print()\n",
    "        print(\"Epoch {} complete! Validation Loss : {}\".format(epoch+1, val_loss))\n",
    "\n",
    "    #     if val_loss < best_loss:\n",
    "    #         print(\"Best validation loss improved from {} to {}\".format(best_loss, val_loss))\n",
    "    #         print()\n",
    "    #         net_copy = copy.deepcopy(net)  # 保存最优模型\n",
    "    #         best_loss = val_loss\n",
    "    #         best_ep = ep + 1\n",
    "\n",
    "    # # 保存模型\n",
    "    # path_to_model='models/{}_lr_{}_val_loss_{}_ep_{}.pt'.format(bert_model, lr, round(best_loss, 5), best_ep)\n",
    "    # torch.save(net_copy.state_dict(), path_to_model)\n",
    "    # print(\"The model has been saved in {}\".format(path_to_model))\n",
    "\n",
    "    del loss\n",
    "    torch.cuda.empty_cache() # 清空显存"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 超参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = \"albert-base-v2\"  # 'albert-base-v2', 'albert-large-v2', 'albert-xlarge-v2', 'albert-xxlarge-v2', 'bert-base-uncased', ...\n",
    "freeze_bert = False  # 是否冻结Bert\n",
    "maxlen = 128  # 最大长度\n",
    "bs = 16  # batch size\n",
    "iters_to_accumulate = 2  # 梯度累加\n",
    "lr = 2e-5  # learning rate\n",
    "epochs = 4  # 训练轮数"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 训练与评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading training data...\n",
      "Reading validation data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/yunchang/anaconda3/envs/pytorch/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      " 21%|██        | 48/230 [00:04<00:15, 11.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 46/230 of epoch 1 complete. Loss : 0.30363395648158115 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 94/230 [00:07<00:11, 11.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 92/230 of epoch 1 complete. Loss : 0.2953830973609634 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 140/230 [00:11<00:07, 12.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 138/230 of epoch 1 complete. Loss : 0.2616934980387273 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 186/230 [00:15<00:03, 12.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 184/230 of epoch 1 complete. Loss : 0.2200546169086643 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 230/230 [00:19<00:00, 12.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 230/230 of epoch 1 complete. Loss : 0.1842898645478746 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 108/108 [00:03<00:00, 28.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 complete! Validation Loss : 0.3846226919580389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 48/230 [00:03<00:15, 12.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 46/230 of epoch 2 complete. Loss : 0.20125033997971079 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 94/230 [00:07<00:11, 12.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 92/230 of epoch 2 complete. Loss : 0.18559055305693462 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 140/230 [00:11<00:07, 12.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 138/230 of epoch 2 complete. Loss : 0.16517505827157394 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 186/230 [00:15<00:03, 11.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 184/230 of epoch 2 complete. Loss : 0.1281280247899501 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 230/230 [00:19<00:00, 12.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 230/230 of epoch 2 complete. Loss : 0.09792480261429497 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 108/108 [00:03<00:00, 29.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 complete! Validation Loss : 0.34365476502312553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 48/230 [00:03<00:14, 12.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 46/230 of epoch 3 complete. Loss : 0.1172106536510198 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 94/230 [00:07<00:10, 12.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 92/230 of epoch 3 complete. Loss : 0.10224637619989074 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 140/230 [00:11<00:07, 12.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 138/230 of epoch 3 complete. Loss : 0.09250902991903864 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 186/230 [00:14<00:03, 12.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 184/230 of epoch 3 complete. Loss : 0.07318819148223037 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 230/230 [00:18<00:00, 12.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 230/230 of epoch 3 complete. Loss : 0.03886690130457282 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 108/108 [00:03<00:00, 29.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 complete! Validation Loss : 0.39413672244107284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 48/230 [00:03<00:14, 12.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 46/230 of epoch 4 complete. Loss : 0.058210089944465006 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 94/230 [00:07<00:10, 12.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 92/230 of epoch 4 complete. Loss : 0.058808597857537476 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 140/230 [00:11<00:07, 12.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 138/230 of epoch 4 complete. Loss : 0.0464486267377177 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 186/230 [00:14<00:03, 12.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 184/230 of epoch 4 complete. Loss : 0.040330378682879 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 230/230 [00:18<00:00, 12.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 230/230 of epoch 4 complete. Loss : 0.02078486543715648 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 108/108 [00:03<00:00, 29.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4 complete! Validation Loss : 0.39935404448597517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#  固定随机种子 便于复现\n",
    "set_seed(1102)\n",
    "\n",
    "# 创建训练集与验证集\n",
    "print(\"Reading training data...\")\n",
    "train_set = CustomDataset(df_train, maxlen, bert_model)\n",
    "print(\"Reading validation data...\")\n",
    "val_set = CustomDataset(df_val, maxlen, bert_model)\n",
    "\n",
    "# 常见训练集与验证集DataLoader\n",
    "train_loader = DataLoader(train_set, batch_size=bs, num_workers=0)\n",
    "val_loader = DataLoader(val_set, batch_size=bs, num_workers=0)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "net = SentencePairClassifier(bert_model, freeze_bert=freeze_bert)\n",
    "\n",
    "# if torch.cuda.device_count() > 1:  # if multiple GPUs\n",
    "#     print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "#     net = nn.DataParallel(net)\n",
    "\n",
    "net.to(device)\n",
    "\n",
    "loss_fun = nn.BCEWithLogitsLoss()\n",
    "opti = AdamW(net.parameters(), lr=lr, weight_decay=1e-2)\n",
    "num_warmup_steps = 0 # The number of steps for the warmup phase.\n",
    "num_training_steps = epochs * len(train_loader)  # The total number of training steps\n",
    "t_total = (len(train_loader) // iters_to_accumulate) * epochs  # Necessary to take into account Gradient accumulation\n",
    "lr_scheduler = get_linear_schedule_with_warmup(optimizer=opti, num_warmup_steps=num_warmup_steps, num_training_steps=t_total)\n",
    "\n",
    "train_bert(net, loss_fun, opti, lr, lr_scheduler, train_loader, val_loader, epochs, iters_to_accumulate, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
